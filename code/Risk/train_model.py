# I want to train a model like a simple NN with 1 hidden layer, 1d input and 1d output to be trained on the samples 
# generated by the sample.py program. So that y=f(x)
# I want to use the Adam optimizer and a mean squared error loss function.
# I want to study the behaviour as the number of samples increases.

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from sample import metropolis_hastings

# Simple neural network with 1 hidden layer
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.hidden = nn.Linear(1, 50)  # 1 input -> 50 hidden neurons
        self.output = nn.Linear(50, 1)  # 50 hidden neurons -> 1 output
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.hidden(x))
        return self.output(x)

def train_and_plot(num_samples):
    # Generate samples
    samples = metropolis_hastings(num_samples)
    X = torch.FloatTensor(samples[:, 0]).reshape(-1, 1)
    y = torch.FloatTensor(samples[:, 1]).reshape(-1, 1)
    
    # Create and train model
    model = SimpleNet()
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    
    # Train for 1000 epochs
    for epoch in range(1000):
        optimizer.zero_grad()
        outputs = model(X)
        loss = criterion(outputs, y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 200 == 0:
            print(f'Samples: {num_samples}, Epoch: {epoch+1}, Loss: {loss.item():.4f}')
    
    # Plot the results
    with torch.no_grad():
        # Sort points for smooth plotting
        sorted_idx = torch.argsort(X.squeeze())
        X_sorted = X[sorted_idx]
        y_sorted = y[sorted_idx]
        y_pred = model(X_sorted)
        
        plt.scatter(X.numpy(), y.numpy(), alpha=0.3, label='Samples')
        plt.plot(X_sorted.numpy(), y_pred.numpy(), 'r-', label='Learned f(x)', linewidth=2)
        plt.plot(X_sorted.numpy(), X_sorted.numpy(), 'g--', label='f(x)=x', linewidth=2)
        plt.title(f'Learned Function with {num_samples} Samples')
        plt.xlabel('x')
        plt.ylabel('y')
        plt.legend()
        plt.grid(True)
        plt.show()

def main():
    # Try different numbers of samples
    for num_samples in [100, 1000, 10000]:
        train_and_plot(num_samples)
        plt.close()

if __name__ == "__main__":
    main()

